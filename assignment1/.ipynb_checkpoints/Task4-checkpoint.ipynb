{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tqdm\n",
    "import utils\n",
    "import dataloaders\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from trainer import Trainer\n",
    "\n",
    "from task2ab import greyscale\n",
    "from task2ab import save_im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "class FullyConnectedModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We are using 28x28 greyscale images.\n",
    "        num_input_nodes = 28*28\n",
    "        # Number of classes in the MNIST dataset\n",
    "        num_classes = 10\n",
    "\n",
    "        # Define our model\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_input_nodes, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Runs a forward pass on the images\n",
    "        x = x.view(-1, 28*28)\n",
    "        out = self.classifier(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters & Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = .0192\n",
    "num_epochs = 5\n",
    "\n",
    "# Parameters for normalization\n",
    "mean = 0.5\n",
    "std = 0.25\n",
    "\n",
    "\n",
    "# Use CrossEntropyLoss for multi-class classification\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Model definition\n",
    "model = FullyConnectedModel()\n",
    "\n",
    "# Define optimizer (Stochastic Gradient Descent)\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr=learning_rate)\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((mean,), (std,))\n",
    "])\n",
    "dataloader_train, dataloader_val = dataloaders.load_dataset(batch_size, image_transform=image_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0: 100%|██████████| 938/938 [00:18<00:00, 51.65it/s]\n",
      "Training epoch 1: 100%|██████████| 938/938 [00:17<00:00, 54.19it/s]\n",
      "Training epoch 2: 100%|██████████| 938/938 [00:18<00:00, 51.67it/s]\n",
      "Training epoch 3: 100%|██████████| 938/938 [00:16<00:00, 57.00it/s]\n",
      "Training epoch 4: 100%|██████████| 938/938 [00:16<00:00, 55.91it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  dataloader_train=dataloader_train,\n",
    "  dataloader_val=dataloader_val,\n",
    "  batch_size=batch_size,\n",
    "  loss_function=loss_function,\n",
    "  optimizer=optimizer\n",
    ")\n",
    "train_loss_dict, val_loss_dict = trainer.train(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'png' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cd645aeddc99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpixels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_weight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight_image'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'png' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot weights as 28 x 28 greyscale images\n",
    "pixels = 28\n",
    "\n",
    "weight = next(model.classifier.children()).weight.data\n",
    "\n",
    "numbers = weight.shape[0]\n",
    "\n",
    "for num in range(numbers):\n",
    "    im = np.zeros((pixels, pixels))\n",
    "    min_weight = weight[num, :].min()\n",
    "    max_weight = weight[num, :].max()\n",
    "    \n",
    "    for row in range(pixels):\n",
    "        for col in range(pixels):\n",
    "            im[row, col] = float((weight[num, row * pixels + col] - min_weight)/(max_weight - min_weight))\n",
    "    plt.imsave(\"weight_ \"+ str(num) + \"_image.jpg\", im, cmap=\"gray\")\n",
    "           \n",
    "\n",
    "# Load\n",
    "prev_train_loss_dict = np.load('prev_train_loss.npy').item()\n",
    "prev_val_loss_dict = np.load('prev_val_loss.npy').item()\n",
    "\n",
    "# Plot loss\n",
    "utils.plot_loss(prev_train_loss_dict, label=\"Train Loss without normalize\")\n",
    "utils.plot_loss(prev_val_loss_dict, label=\"Test Loss without normalize\")\n",
    "\n",
    "utils.plot_loss(train_loss_dict, label=\"Train Loss with normalize\")\n",
    "utils.plot_loss(val_loss_dict, label=\"Test Loss with normalize\")\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Images Seen\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.savefig(\"training_loss.png\")\n",
    "\n",
    "plt.show()\n",
    "torch.save(model.state_dict(), \"saved_model.torch\")\n",
    "\n",
    "final_loss, final_acc = utils.compute_loss_and_accuracy(\n",
    "    dataloader_val, model, loss_function)\n",
    "print(f\"Final Test Cross Entropy Loss: {final_loss}. Final Test accuracy: {final_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tdt4195)",
   "language": "python",
   "name": "tdt4195"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
